{
  
    
        "post0": {
            "title": "Soda First Take",
            "content": "Soda First Take . Programmatic access: Soda SQL . Web-app access: Soda Cloud . Soda SQL: . Connect data source . Seems to be only able to connect with database (not a single file) . | Users need to edit warehouse.yml and env_vars.yml manually (one is database access details like port, one is credentials) . Different yaml files for different schema databases | | | Must do a first scan (similar to our data registration) . Scan yaml needs to define table, metrics to be used, test to be operated or optionally the columns in the table that will be tested . Different scan yaml files for different datasets within the same database | | | Then we can apply default metrics or user-defined metrics to columns. (user-defined metrics are more like SQL queries). It also needs to submit a scan yaml file to do the jobs. . | I tried to followed the instructions to use soda SQL, but I ran into errors. I got a couple warnings when I did “pip install soda-sql-postgresql” but it still went through. When I tried to use “soda create” to create the data warehouse yaml file. It failed. So was the env_var yaml file. Therefore, I manually created them. Then run “soda analyze” but it still couldn’t go through. I think it has to do with the yaml file parser. . After failed to use my own dataset. I tried to use their tutorial dataset which was containerized. And it still wouldn’t work for me. No containers were running with no errors. . . . Soda Cloud: . They have a clean UI. However, I was not able to connect to their tutorial database neither. . | . | According to the documentation, users can edit data source, edit tests, and define metrics when creating new monitors. . | Metrics: . I can find their files and view them in Pycharm or Notepad++ but I didn’t understand them. The hierarchical architecture about different modules is a bit complex for me. It would take me a while to understand them. . .",
            "url": "https://itsming97.github.io/how-to-be-rengoku-diary/2021/08/02/Soda-First-Take.html",
            "relUrl": "/2021/08/02/Soda-First-Take.html",
            "date": " • Aug 2, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "First",
            "content": "My first blog . Monday: . I set up the spark environment on my local device. . I first installed the Java 8. Many people reported that later Java version like 9 or 10 are not compatible with spark. . | Then I installed Python (the latest version) on my machine. Before installing python, I mostly ran python on Anaconda environment using jupyter notebook, and I ran a few py files using Spyder. I am not very sure about the differences, but before downloading python, I ran command line “python –version” on windows prompt and there was no output. There was an important step during the python installation is to add python to PATH which we can find and edit the system environment variables. . | Then I installed spark from apache spark official website, also the latest version. I learned that checking the software file checksum is a good way to verify the integrity of the download before installing. . | Then I added winutils.exe file for the underlying Hadoop version to my desired folder. . | Then I configured my environment variables. Mainly I added the spark, Hadoop, and java to PATH as I did to python during installation. . | Lastly, I launched spark. By calling spark-shell, I was able to run spark with scala. By calling pyspark, I was able to run spark with python. . | Two bugs took me most of the time. One is that “WARN ProcfsMetricsGetter:Exception when trying to compute pagesize” error, I didn’t solve this problem as I found that this is a minor bug that does not affect the performance and it should be just some metric issues. The second bug occurred when I tried to use pyspark. It is a bug involves JavaError, Python and py4j keywords. I tried a couple methods including downloading py4j, editing the environment variables on PATH, calling different files to launch Python, managing app execution aliases, etc. I solved it by setting the environment variable (PYSPARK_PYTHON = /path/python) on window prompt before calling the pyspark file. (However, at the moment I am writing this blog. It does not work. I will try to figure out tomorrow.) Anyway, they all worked on Monday. . | . Tuesday: . Basically I didn’t do anything because the repair took me whole day. . Wednesday: . I installed Journal. I followed the instruction although I could not understand it completely. What I learned the most from the process is having a better idea about what docker is and how to use docker. I successfully downloaded the Journal image and run the container. However, I don’t know how to use the container. I still don’t know how to run the Journal command line. . | I also implemented fastpages for the my blog. I didn’t change much content and interface of the blog and add additional designs to it yet as docker took me most of the time. . | .",
            "url": "https://itsming97.github.io/how-to-be-rengoku-diary/2021/07/28/first.html",
            "relUrl": "/2021/07/28/first.html",
            "date": " • Jul 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://itsming97.github.io/how-to-be-rengoku-diary/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://itsming97.github.io/how-to-be-rengoku-diary/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}