{
  
    
        "post0": {
            "title": "Delta _lake01",
            "content": "Delta Lake 01 . I examinate how to load data in and out from my local machine to delta lake. As far as I understand, delta lake is built on top of spark. It is more like a special format or special class for me. Most importantly, it provides ACID. I ran all the codes on windows power shell as I set up the environment there. This notebook is in the anaconda environment. I tried to run the same codes here so that we can see output as well. However, the configuration took me a while and it still failed so I used the markdown format. . Run PySpark with the Delta Lake package and additional configurations: . pyspark –packages io.delta:delta-core_2.12:1.0.0 –conf “spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension” –conf “spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog” . Then I load the data into spark: . data = spark.read.load(“path to the local file”) . pq=data.write.format(“delta”).save(“/tmp/pq-table”) . df = spark.read.format(“delta”).load(“/tmp/pq-table”) . The /tmp is a directory to save temporary files. pq-table is a delta object. Names have to be unique. For this particular file, one of the columns has a space in its name. The space or other special characters will stop the table becoming a delta table. Therefore, I dropped the column just because of convenience. I tried .withColumnRenamed() but it didn’t work. I should try alias, but I am still learning that. . Partition and write the dataset: . df.write . .format(“delta”) . .partitionBy(“Date”) . .saveAsTable(“stock”) . The codes above would write the dataset from spark to local machine at the current directory. PartitionBy would create different sub-directories under the current directory. However, each unique value is one directory. I extracted year and month out and put them into PartitionBy, but they don’t work. If we want nested directories like below (example). We might have to create the year and month columns, but I know that it would be redundant and that’s not what we want. I would keep looking for solutions for this. . We can also use bucketing, but a buckets number has to be assigned in that function, so I think it is not ideal. . Example: . Root: . –year1 | . —-Month1 | . ——day1 | . Output of the write: . . Next Step: . Keep looking for solutions to address how to write the dataset out into nested folders based on column . | Learn how to move data from cloud platform/database connections/API to spark and then partition out to cloud platform. Like S3 -&gt; Spark -&gt; S3 . | How to apply python profiling to the delta table . | . Questions: . Relationship between Spark and Delta Lake. Like how does the parquet file move to spark then move to Delta Lake? Or is it simply changing the spark dataframe into delta format? Like change it to a different class in python. I understand that Delta Lake would provide ACID which spark does not have. It makes sense if ACID is critical to our goal. However, if we use Delta Lake just because of partitioning. I think the original spark already has the partitioning function. . | Need to figure out all these different paths. Some are cloud path and some are local paths. How to access these directories and manage the files in those directory. . | .",
            "url": "https://itsming97.github.io/how-to-be-rengoku-diary/2021/08/06/Delta-_Lake01.html",
            "relUrl": "/2021/08/06/Delta-_Lake01.html",
            "date": " • Aug 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Scaffold_taxi.demo",
            "content": "Scaffold a new Expectation Suite (Experimental) . This process helps you avoid writing lots of boilerplate when authoring suites by allowing you to select columns and other factors that you care about and letting a profiler write some candidate expectations for you to adjust. . Expectation Suite Name: taxi.demo . We’d love it if you’d reach out to us on the Great Expectations Slack Channel! . import great_expectations as ge from great_expectations.checkpoint import LegacyCheckpoint from great_expectations.profile.user_configurable_profiler import UserConfigurableProfiler from great_expectations.data_context.types.resource_identifiers import ValidationResultIdentifier context = ge.data_context.DataContext() expectation_suite_name = &quot;taxi.demo&quot; # Wipe the suite clean to prevent unwanted expectations in the batch suite = context.create_expectation_suite(expectation_suite_name, overwrite_existing=True) batch_kwargs = {&#39;path&#39;: &#39;D: gathi great_expecatation ge_tutorials great_expectations .. data yellow_tripdata_sample_2019-01.csv&#39;, &#39;datasource&#39;: &#39;data__dir&#39;, &#39;data_asset_name&#39;: &#39;yellow_tripdata_sample_2019-01&#39;} batch = context.get_batch(batch_kwargs, suite) batch.head() . c: python python39 lib site-packages great_expectations expectations metrics column_aggregate_metrics column_quantile_values.py:3: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated since Python 3.3, and in 3.10 it will stop working from collections import Iterable . vendor_id pickup_datetime dropoff_datetime passenger_count trip_distance rate_code_id store_and_fwd_flag pickup_location_id dropoff_location_id payment_type fare_amount extra mta_tax tip_amount tolls_amount improvement_surcharge total_amount congestion_surcharge . 0 1 | 2019-01-15 03:36:12 | 2019-01-15 03:42:19 | 1 | 1.0 | 1 | N | 230 | 48 | 1 | 6.5 | 0.5 | 0.5 | 1.95 | 0.0 | 0.3 | 9.75 | NaN | . 1 1 | 2019-01-25 18:20:32 | 2019-01-25 18:26:55 | 1 | 0.8 | 1 | N | 112 | 112 | 1 | 6.0 | 1.0 | 0.5 | 1.55 | 0.0 | 0.3 | 9.35 | 0.0 | . 2 1 | 2019-01-05 06:47:31 | 2019-01-05 06:52:19 | 1 | 1.1 | 1 | N | 107 | 4 | 2 | 6.0 | 0.0 | 0.5 | 0.00 | 0.0 | 0.3 | 6.80 | NaN | . 3 1 | 2019-01-09 15:08:02 | 2019-01-09 15:20:17 | 1 | 2.5 | 1 | N | 143 | 158 | 1 | 11.0 | 0.0 | 0.5 | 3.00 | 0.0 | 0.3 | 14.80 | NaN | . 4 1 | 2019-01-25 18:49:51 | 2019-01-25 18:56:44 | 1 | 0.8 | 1 | N | 246 | 90 | 1 | 6.5 | 1.0 | 0.5 | 1.65 | 0.0 | 0.3 | 9.95 | 0.0 | . Select the columns on which you would like to scaffold expectations and those which you would like to ignore. . Great Expectations will choose which expectations might make sense for a column based on the data type and cardinality of the data in each selected column. . Simply comment out columns that are important and should be included. You can select multiple lines and use a jupyter keyboard shortcut to toggle each line: Linux/Windows: Ctrl-/, macOS: Cmd-/ . ignored_columns = [ &#39;vendor_id&#39;, &#39;pickup_datetime&#39;, &#39;dropoff_datetime&#39;, #&#39;passenger_count&#39;, &#39;trip_distance&#39;, &#39;rate_code_id&#39;, &#39;store_and_fwd_flag&#39;, &#39;pickup_location_id&#39;, &#39;dropoff_location_id&#39;, &#39;payment_type&#39;, &#39;fare_amount&#39;, &#39;extra&#39;, &#39;mta_tax&#39;, &#39;tip_amount&#39;, &#39;tolls_amount&#39;, &#39;improvement_surcharge&#39;, &#39;total_amount&#39;, &#39;congestion_surcharge&#39; ] . Run the scaffolder . The suites generated here are not meant to be production suites - they are scaffolds to build upon. . To get to a production grade suite, you will definitely want to edit this suite after scaffolding gets you close to what you want. . This is highly configurable depending on your goals. You can ignore columns or exclude certain expectations, specify a threshold for creating value set expectations, or even specify semantic types for a given column. You can find more information about how to configure this profiler, including a list of the expectations that it uses, here. . profiler = UserConfigurableProfiler(profile_dataset=batch, ignored_columns=ignored_columns, excluded_expectations=None, not_null_only=False, primary_or_compound_key=False, semantic_types_dict=None, table_expectations_only=False, value_set_threshold=&quot;MANY&quot;, ) suite = profiler.build_suite() . Profiling: 0%| | 0/1 [00:00&lt;?, ?it/s, Column=passenger_count] Creating an expectation suite with the following expectations: Table-Level Expectations expect_table_columns_to_match_ordered_list expect_table_row_count_to_be_between Expectations by Column Column Name: passenger_count | Column Data Type: INT | Cardinality: VERY_FEW expect_column_max_to_be_between expect_column_mean_to_be_between expect_column_median_to_be_between expect_column_min_to_be_between expect_column_proportion_of_unique_values_to_be_between expect_column_quantile_values_to_be_between expect_column_values_to_be_in_set expect_column_values_to_be_in_type_list expect_column_values_to_not_be_null . Save &amp; review the scaffolded Expectation Suite . Let’s save the scaffolded expectation suite as a JSON file in the great_expectations/expectations directory of your project and rebuild the Data Docs site to make it easy to review the scaffolded suite. . context.save_expectation_suite(suite, expectation_suite_name) results = LegacyCheckpoint( name=&quot;_temp_checkpoint&quot;, data_context=context, batches=[ { &quot;batch_kwargs&quot;: batch_kwargs, &quot;expectation_suite_names&quot;: [expectation_suite_name] } ] ).run() validation_result_identifier = results.list_validation_result_identifiers()[0] context.build_data_docs() context.open_data_docs(validation_result_identifier) . c: python python39 lib site-packages jinja2 environment.py:1088: DeprecationWarning: &#39;soft_unicode&#39; has been renamed to &#39;soft_str&#39;. The old name will be removed in MarkupSafe 2.1. return concat(self.root_render_func(self.new_context(vars))) . Next steps . After you review this scaffolded Expectation Suite in Data Docs you should edit this suite to make finer grained adjustments to the expectations. This can be done by running great_expectations suite edit taxi.demo. . batch_kwargs_2 = {&#39;path&#39;: &#39;D: gathi great_expecatation ge_tutorials great_expectations .. data yellow_tripdata_sample_2019-02.csv&#39;, &#39;datasource&#39;: &#39;data__dir&#39;, &#39;data_asset_name&#39;: &#39;yellow_tripdata_sample_2019-02&#39;} my_checkpoint = LegacyCheckpoint( name=&quot;my_checkpoint&quot;, data_context=context, batches=[ { &quot;batch_kwargs&quot;: batch_kwargs_2, &quot;expectation_suite_names&quot;: [&quot;taxi.demo&quot;] } ] ) results = my_checkpoint.run() . c: python python39 lib site-packages jinja2 environment.py:1088: DeprecationWarning: &#39;soft_unicode&#39; has been renamed to &#39;soft_str&#39;. The old name will be removed in MarkupSafe 2.1. return concat(self.root_render_func(self.new_context(vars))) . # Save the Checkpoint to your Data Context my_checkpoint_json = my_checkpoint.config.to_json_dict() context.add_checkpoint(**my_checkpoint_json) . &lt;great_expectations.checkpoint.checkpoint.LegacyCheckpoint at 0x1b4b6d56760&gt; . # And here&#39;s how you can load it from your Data Context again my_loaded_checkpoint = context.get_checkpoint(&quot;my_checkpoint&quot;) # And then run validation again if you&#39;d like my_loaded_checkpoint.run() . c: python python39 lib site-packages jinja2 environment.py:1088: DeprecationWarning: &#39;soft_unicode&#39; has been renamed to &#39;soft_str&#39;. The old name will be removed in MarkupSafe 2.1. return concat(self.root_render_func(self.new_context(vars))) { &quot;run_results&quot;: { &quot;ValidationResultIdentifier::taxi/demo/20210804T182440.545826Z/20210804T182440.545826Z/8479f9ad9e8961a6bac205d71c08618c&quot;: { &quot;validation_result&quot;: { &quot;evaluation_parameters&quot;: {}, &quot;results&quot;: [ { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: true, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column_list&quot;: [ &quot;vendor_id&quot;, &quot;pickup_datetime&quot;, &quot;dropoff_datetime&quot;, &quot;passenger_count&quot;, &quot;trip_distance&quot;, &quot;rate_code_id&quot;, &quot;store_and_fwd_flag&quot;, &quot;pickup_location_id&quot;, &quot;dropoff_location_id&quot;, &quot;payment_type&quot;, &quot;fare_amount&quot;, &quot;extra&quot;, &quot;mta_tax&quot;, &quot;tip_amount&quot;, &quot;tolls_amount&quot;, &quot;improvement_surcharge&quot;, &quot;total_amount&quot;, &quot;congestion_surcharge&quot; ], &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_table_columns_to_match_ordered_list&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: [ &quot;vendor_id&quot;, &quot;pickup_datetime&quot;, &quot;dropoff_datetime&quot;, &quot;passenger_count&quot;, &quot;trip_distance&quot;, &quot;rate_code_id&quot;, &quot;store_and_fwd_flag&quot;, &quot;pickup_location_id&quot;, &quot;dropoff_location_id&quot;, &quot;payment_type&quot;, &quot;fare_amount&quot;, &quot;extra&quot;, &quot;mta_tax&quot;, &quot;tip_amount&quot;, &quot;tolls_amount&quot;, &quot;improvement_surcharge&quot;, &quot;total_amount&quot;, &quot;congestion_surcharge&quot; ] }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: true, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;max_value&quot;: 10000, &quot;min_value&quot;: 10000, &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_table_row_count_to_be_between&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: 10000 }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: false, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column&quot;: &quot;passenger_count&quot;, &quot;max_value&quot;: 1, &quot;min_value&quot;: 1, &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_min_to_be_between&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: 0, &quot;element_count&quot;: 10000, &quot;missing_count&quot;: null, &quot;missing_percent&quot;: null }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: true, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column&quot;: &quot;passenger_count&quot;, &quot;max_value&quot;: 6, &quot;min_value&quot;: 6, &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_max_to_be_between&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: 6, &quot;element_count&quot;: 10000, &quot;missing_count&quot;: null, &quot;missing_percent&quot;: null }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: false, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column&quot;: &quot;passenger_count&quot;, &quot;max_value&quot;: 1.5716, &quot;min_value&quot;: 1.5716, &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_mean_to_be_between&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: 1.3577, &quot;element_count&quot;: 10000, &quot;missing_count&quot;: null, &quot;missing_percent&quot;: null }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: true, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column&quot;: &quot;passenger_count&quot;, &quot;max_value&quot;: 1.0, &quot;min_value&quot;: 1.0, &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_median_to_be_between&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: 1.0, &quot;element_count&quot;: 10000, &quot;missing_count&quot;: null, &quot;missing_percent&quot;: null }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: false, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;allow_relative_error&quot;: &quot;lower&quot;, &quot;column&quot;: &quot;passenger_count&quot;, &quot;quantile_ranges&quot;: { &quot;quantiles&quot;: [ 0.05, 0.25, 0.5, 0.75, 0.95 ], &quot;value_ranges&quot;: [ [ 1, 1 ], [ 1, 1 ], [ 1, 1 ], [ 2, 2 ], [ 5, 5 ] ] }, &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_quantile_values_to_be_between&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: { &quot;quantiles&quot;: [ 0.05, 0.25, 0.5, 0.75, 0.95 ], &quot;values&quot;: [ 0, 1, 1, 1, 5 ] }, &quot;element_count&quot;: 10000, &quot;missing_count&quot;: null, &quot;missing_percent&quot;: null, &quot;details&quot;: { &quot;success_details&quot;: [ false, true, true, false, true ] } }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: false, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column&quot;: &quot;passenger_count&quot;, &quot;value_set&quot;: [ 1, 2, 3, 4, 5, 6 ], &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_values_to_be_in_set&quot; }, &quot;result&quot;: { &quot;element_count&quot;: 10000, &quot;missing_count&quot;: 0, &quot;missing_percent&quot;: 0.0, &quot;unexpected_count&quot;: 1579, &quot;unexpected_percent&quot;: 15.790000000000001, &quot;unexpected_percent_total&quot;: 15.790000000000001, &quot;unexpected_percent_nonmissing&quot;: 15.790000000000001, &quot;partial_unexpected_list&quot;: [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ], &quot;partial_unexpected_index_list&quot;: [ 14, 24, 78, 101, 141, 160, 162, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185 ], &quot;partial_unexpected_counts&quot;: [ { &quot;value&quot;: 0, &quot;count&quot;: 1579 } ] }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: true, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column&quot;: &quot;passenger_count&quot;, &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_values_to_not_be_null&quot; }, &quot;result&quot;: { &quot;element_count&quot;: 10000, &quot;unexpected_count&quot;: 0, &quot;unexpected_percent&quot;: 0.0, &quot;unexpected_percent_total&quot;: 0.0, &quot;partial_unexpected_list&quot;: [] }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: false, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column&quot;: &quot;passenger_count&quot;, &quot;max_value&quot;: 0.0006, &quot;min_value&quot;: 0.0006, &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_proportion_of_unique_values_to_be_between&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: 0.0007, &quot;element_count&quot;: 10000, &quot;missing_count&quot;: null, &quot;missing_percent&quot;: null }, &quot;meta&quot;: {} }, { &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_message&quot;: null, &quot;exception_traceback&quot;: null }, &quot;success&quot;: true, &quot;expectation_config&quot;: { &quot;kwargs&quot;: { &quot;column&quot;: &quot;passenger_count&quot;, &quot;type_list&quot;: [ &quot;INTEGER&quot;, &quot;integer&quot;, &quot;int&quot;, &quot;int_&quot;, &quot;int8&quot;, &quot;int16&quot;, &quot;int32&quot;, &quot;int64&quot;, &quot;uint8&quot;, &quot;uint16&quot;, &quot;uint32&quot;, &quot;uint64&quot;, &quot;INT&quot;, &quot;INTEGER&quot;, &quot;INT64&quot;, &quot;TINYINT&quot;, &quot;BYTEINT&quot;, &quot;SMALLINT&quot;, &quot;BIGINT&quot;, &quot;IntegerType&quot;, &quot;LongType&quot; ], &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } }, &quot;meta&quot;: {}, &quot;expectation_type&quot;: &quot;expect_column_values_to_be_in_type_list&quot; }, &quot;result&quot;: { &quot;observed_value&quot;: &quot;int64&quot; }, &quot;meta&quot;: {} } ], &quot;success&quot;: false, &quot;statistics&quot;: { &quot;evaluated_expectations&quot;: 11, &quot;successful_expectations&quot;: 6, &quot;unsuccessful_expectations&quot;: 5, &quot;success_percent&quot;: 54.54545454545454 }, &quot;meta&quot;: { &quot;great_expectations_version&quot;: &quot;0.13.25&quot;, &quot;expectation_suite_name&quot;: &quot;taxi.demo&quot;, &quot;run_id&quot;: { &quot;run_name&quot;: &quot;20210804T182440.545826Z&quot;, &quot;run_time&quot;: &quot;2021-08-04T18:24:40.545826+00:00&quot; }, &quot;batch_kwargs&quot;: { &quot;path&quot;: &quot;D: gathi great_expecatation ge_tutorials great_expectations .. data yellow_tripdata_sample_2019-02.csv&quot;, &quot;datasource&quot;: &quot;data__dir&quot;, &quot;data_asset_name&quot;: &quot;yellow_tripdata_sample_2019-02&quot; }, &quot;batch_markers&quot;: { &quot;ge_load_time&quot;: &quot;20210804T182440.507853Z&quot;, &quot;pandas_data_fingerprint&quot;: &quot;88b447d903f05fb594b87b13de399e45&quot; }, &quot;batch_parameters&quot;: null, &quot;validation_time&quot;: &quot;20210804T182440.551826Z&quot;, &quot;expectation_suite_meta&quot;: { &quot;columns&quot;: { &quot;congestion_surcharge&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;dropoff_datetime&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;dropoff_location_id&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;extra&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;fare_amount&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;improvement_surcharge&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;mta_tax&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;passenger_count&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;payment_type&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;pickup_datetime&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;pickup_location_id&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;rate_code_id&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;store_and_fwd_flag&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;tip_amount&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;tolls_amount&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;total_amount&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;trip_distance&quot;: { &quot;description&quot;: &quot;&quot; }, &quot;vendor_id&quot;: { &quot;description&quot;: &quot;&quot; } }, &quot;great_expectations_version&quot;: &quot;0.13.25&quot; } } }, &quot;actions_results&quot;: { &quot;store_validation_result&quot;: { &quot;class&quot;: &quot;StoreValidationResultAction&quot; }, &quot;store_evaluation_params&quot;: { &quot;class&quot;: &quot;StoreEvaluationParametersAction&quot; }, &quot;update_data_docs&quot;: { &quot;local_site&quot;: &quot;file://D: gathi great_expecatation ge_tutorials great_expectations uncommitted/data_docs/local_site/validations taxi demo 20210804T182440.545826Z 20210804T182440.545826Z 8479f9ad9e8961a6bac205d71c08618c.html&quot;, &quot;class&quot;: &quot;UpdateDataDocsAction&quot; } } } }, &quot;evaluation_parameters&quot;: null, &quot;success&quot;: false, &quot;run_id&quot;: { &quot;run_name&quot;: &quot;20210804T182440.545826Z&quot;, &quot;run_time&quot;: &quot;2021-08-04T18:24:40.545826+00:00&quot; }, &quot;validation_operator_config&quot;: { &quot;class_name&quot;: &quot;ActionListValidationOperator&quot;, &quot;module_name&quot;: &quot;great_expectations.validation_operators&quot;, &quot;name&quot;: &quot;default-action-list-validation-operator&quot;, &quot;kwargs&quot;: { &quot;action_list&quot;: [ { &quot;name&quot;: &quot;store_validation_result&quot;, &quot;action&quot;: { &quot;class_name&quot;: &quot;StoreValidationResultAction&quot; } }, { &quot;name&quot;: &quot;store_evaluation_params&quot;, &quot;action&quot;: { &quot;class_name&quot;: &quot;StoreEvaluationParametersAction&quot; } }, { &quot;name&quot;: &quot;update_data_docs&quot;, &quot;action&quot;: { &quot;class_name&quot;: &quot;UpdateDataDocsAction&quot;, &quot;site_names&quot;: [] } } ], &quot;result_format&quot;: { &quot;result_format&quot;: &quot;SUMMARY&quot;, &quot;partial_unexpected_count&quot;: 20 } } } } . validation_result_identifier = results.list_validation_result_identifiers()[0] context.build_data_docs() context.open_data_docs(validation_result_identifier) . .",
            "url": "https://itsming97.github.io/how-to-be-rengoku-diary/2021/08/04/scaffold_taxi.demo.html",
            "relUrl": "/2021/08/04/scaffold_taxi.demo.html",
            "date": " • Aug 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Great Expectation First Take",
            "content": "Great Expectation First Take . What is cls? Similar to self but cls is used when the method is a class method . What is @DocInherit? It overrides function’s __get__ method with one that will replace the local docstring with the docstring from its parent. It is defined in Dataset.uitl. . Basically, we need to edit the great_expectations.yml file and push this to the repo and the pipeline or the agent will be able to execute the jobs. Below are the content of the great_expectation.yml. . . The datasources is to define the data source and extract the batch of the data for processing. In the demo, I used files (csv files) in the files system so the class_name is PandasDataset. It works for more than csv file. Parquet, xls, xlsx, json and other common extensions are recognized. However, all batch_kwargs will be extracted out and will be used for creating pandas dataframe for later use (for the cases of local files). . . Config_variables_file_path is the directory for saving config_variables.yml which holds sensitive secrets such as database credential. Plugins is to add more features or for more customizations. . . These stores are directories to store different outcomes or parameters. Expectation_store stores the json files for different suites. The json files explain what expectation types were used. Expectation types are like metrics before. For example, max and min values. And we can set these values and apply to only specific columns. Validation stores validation results. Like how many expectations are tested, how many of them failed and what are the unexpected values in the expectations. Validation results are in json format. Checkpoints are the previous data. Mostly should be the good ones so that we can use them to validate new data. . . Data docs are the html file of the expectation suite results, they are the visualizations of the outcome. There is home page for them and can monitor different projects or suites. Home page is like below. . . Metrics and tests are expectations in this library. The collection of them is an expectation suite. See below. . From my own experiences, it does not work like user drop off a file then the output would come out. It went through some python codes in notebook. It would be better if modifying the pipeline to operate those python modules once the target datafile is pushed to the repo. I will upload the notebook later. .",
            "url": "https://itsming97.github.io/how-to-be-rengoku-diary/2021/08/04/Great-Expectation-First-Take.html",
            "relUrl": "/2021/08/04/Great-Expectation-First-Take.html",
            "date": " • Aug 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Soda Second Take",
            "content": "Soda.io Second Take . Today I continued to set up my soda sql command line interface. . Many different tryouts: . I thought that I didn’t have the postgres database. I checked them and I do have them. By calling . psql I was able to run SQL command lines on windows powershell. . | I re-installed soda-sql-postgresql. There were two errors that kept the installation being successful. First one is the installation of psycopg2-binary. I tried manually installed the library by using pip and re-installed soda sql but it still had the same error. Then I found the second error was about the version of the Visual Studio and the C++ compilers and libraries, etc. So I installed the latest visual studio build tools and install the C++ kit. After all, the soda sql was installed successfully with a couple warnings but no errors. However, the programmatic access still didn’t go through. . | After making sure that all dependencies or other needed packages were installed properly, I started to look at the traceback of the errors. But I don’t know how to debug them. . | After trying multiple solutions but not making it run, I put soda aside and started to look into Great Expectation. I do not have much details for now. I would keep looking at its codes and documentations after dinner. I would include this part into my tomorrow’s blog. .",
            "url": "https://itsming97.github.io/how-to-be-rengoku-diary/2021/08/03/Soda-Second-Take.html",
            "relUrl": "/2021/08/03/Soda-Second-Take.html",
            "date": " • Aug 3, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Soda First Take",
            "content": "Soda First Take . Programmatic access: Soda SQL . Web-app access: Soda Cloud . Soda SQL: . Connect data source . Seems to be only able to connect with database (not a single file) . | Users need to edit warehouse.yml and env_vars.yml manually (one is database access details like port, one is credentials) . Different yaml files for different schema databases | | | Must do a first scan (similar to our data registration) . Scan yaml needs to define table, metrics to be used, test to be operated or optionally the columns in the table that will be tested . Different scan yaml files for different datasets within the same database | | | Then we can apply default metrics or user-defined metrics to columns. (user-defined metrics are more like SQL queries). It also needs to submit a scan yaml file to do the jobs. . | I tried to followed the instructions to use soda SQL, but I ran into errors. I got a couple warnings when I did “pip install soda-sql-postgresql” but it still went through. When I tried to use “soda create” to create the data warehouse yaml file. It failed. So was the env_var yaml file. Therefore, I manually created them. Then run “soda analyze” but it still couldn’t go through. I think it has to do with the yaml file parser. . After failed to use my own dataset. I tried to use their tutorial dataset which was containerized. And it still wouldn’t work for me. No containers were running with no errors. . . . Soda Cloud: . They have a clean UI. However, I was not able to connect to their tutorial database neither. . | . | According to the documentation, users can edit data source, edit tests, and define metrics when creating new monitors. . | Metrics: . I can find their files and view them in Pycharm or Notepad++ but I didn’t understand them. The hierarchical architecture about different modules is a bit complex for me. It would take me a while to understand them. . .",
            "url": "https://itsming97.github.io/how-to-be-rengoku-diary/2021/08/02/Soda-First-Take.html",
            "relUrl": "/2021/08/02/Soda-First-Take.html",
            "date": " • Aug 2, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "First",
            "content": "My first blog . Monday: . I set up the spark environment on my local device. . I first installed the Java 8. Many people reported that later Java version like 9 or 10 are not compatible with spark. . | Then I installed Python (the latest version) on my machine. Before installing python, I mostly ran python on Anaconda environment using jupyter notebook, and I ran a few py files using Spyder. I am not very sure about the differences, but before downloading python, I ran command line “python –version” on windows prompt and there was no output. There was an important step during the python installation is to add python to PATH which we can find and edit the system environment variables. . | Then I installed spark from apache spark official website, also the latest version. I learned that checking the software file checksum is a good way to verify the integrity of the download before installing. . | Then I added winutils.exe file for the underlying Hadoop version to my desired folder. . | Then I configured my environment variables. Mainly I added the spark, Hadoop, and java to PATH as I did to python during installation. . | Lastly, I launched spark. By calling spark-shell, I was able to run spark with scala. By calling pyspark, I was able to run spark with python. . | Two bugs took me most of the time. One is that “WARN ProcfsMetricsGetter:Exception when trying to compute pagesize” error, I didn’t solve this problem as I found that this is a minor bug that does not affect the performance and it should be just some metric issues. The second bug occurred when I tried to use pyspark. It is a bug involves JavaError, Python and py4j keywords. I tried a couple methods including downloading py4j, editing the environment variables on PATH, calling different files to launch Python, managing app execution aliases, etc. I solved it by setting the environment variable (PYSPARK_PYTHON = /path/python) on window prompt before calling the pyspark file. (However, at the moment I am writing this blog. It does not work. I will try to figure out tomorrow.) Anyway, they all worked on Monday. . | . Tuesday: . Basically I didn’t do anything because the repair took me whole day. . Wednesday: . I installed Journal. I followed the instruction although I could not understand it completely. What I learned the most from the process is having a better idea about what docker is and how to use docker. I successfully downloaded the Journal image and run the container. However, I don’t know how to use the container. I still don’t know how to run the Journal command line. . | I also implemented fastpages for the my blog. I didn’t change much content and interface of the blog and add additional designs to it yet as docker took me most of the time. . | .",
            "url": "https://itsming97.github.io/how-to-be-rengoku-diary/2021/07/28/first.html",
            "relUrl": "/2021/07/28/first.html",
            "date": " • Jul 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://itsming97.github.io/how-to-be-rengoku-diary/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://itsming97.github.io/how-to-be-rengoku-diary/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}